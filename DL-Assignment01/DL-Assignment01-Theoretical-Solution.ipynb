{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmubxigEE-Zb"
      },
      "source": [
        "# Assignment 1: Universal Function Approximator\n",
        "\n",
        "\n",
        "The goal of this exercise is to compare three different neural network architectures and analyze their capacity for function approximation:\n",
        "\n",
        "1. $N_1$: One-layer network (linear transformation only)\n",
        "2. $N_2$: One-layer network with non-linear activation function\n",
        "3. $N_3$: Two-layer network (hidden layer with non-linear activation function)\n",
        "\n",
        "They will be trained via gradient descent (with weight decay). To show the flexibility of the approach, three different functions will be approximated:\n",
        "1. $X_1: t = \\cos(3x)$ for $x\\in[-2,2]$\n",
        "2. $X_2: t = e^{-x^2}$ for $x\\in[-1,1]$\n",
        "3. $X_3: t = x^5 + 3x^4 - 6x^3 -12x^2 + 5x + 129$ for $x\\in[-4,2.5]$\n",
        "\n",
        "In the theoretical section, the networks will be designed, and the necessary derivatives will be computed by hand.\n",
        "\n",
        "In the coding section, we will:\n",
        "\n",
        "- implement the networks and their gradients,\n",
        "- generate target data for three different functions,\n",
        "- apply the training procedure to the data, and\n",
        "- plot the resulting approximated function together with the data samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n36PMmPANmJ7"
      },
      "source": [
        "## Section 1: Theoretical Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXFApo7obLKe"
      },
      "source": [
        "### Network Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH8oL0-MQ0IY"
      },
      "source": [
        "#### Task 1.1: Network Structure\n",
        "\n",
        "Given input $\\vec x = (1, x)^T$, define three neural networks ($N_1$, $N_2$, $N_3$) mathematically, to reach output $y$. Use $g()$ to represent the activation function.\n",
        "\n",
        "Explain how their structures differ and analyze their function approximation capabilities.\n",
        "\n",
        "---\n",
        "Note:\n",
        "\n",
        "For one-layer networks, define parameter $\\Theta=\\vec w \\in\\mathbb R^{D+1}$\n",
        "\n",
        "For two-layer network, define parameters $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$ that are split into $\\mathbf W^{(1)}\\in\\mathbb R^{K\\times {(D+1)}}$ for the first layer and $\\vec w^{(2)}\\in\\mathbb R^{K+1}$ for the second layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OopahJtQTe75"
      },
      "source": [
        "Answer:\n",
        "\n",
        "1. $N_1$: A one-layer network is simply a linear mapping: $y=\\vec w^T\\vec x$. This model can only approximate linear functions.\n",
        "\n",
        "2. $N_2$: A one-layer network with activation function applies a non-linearity:\n",
        "\n",
        "   1. Linear layer: $\\vec a=\\vec w^T\\vec x$\n",
        "   2. Apply the activation function element-wise: $y=g(\\vec a)$\n",
        "\n",
        "   This can approximate some non-linear functions but is limited.\n",
        "\n",
        "3. $N_3$: A two-layer network introduces a hidden layer:\n",
        "\n",
        "   1. First layer: $\\vec a_- = \\mathbf W^{(1)} \\vec x$\n",
        "   2. Apply the activation function element-wise: $\\vec a_- : \\vec h_- = g(\\vec a_-)$.\n",
        "   3. Prepend the bias neuron $h_0=1$ to arrive at $\\vec h$\n",
        "   4. Compute the network output: $y = \\vec w^{(2)}{}^T\\vec h$\n",
        "\n",
        "   This allows for more complex function approximation by transforming the input space non-linearly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSm46qR8SdQH"
      },
      "source": [
        "#### Task 1.2: Network Comparison\n",
        "\n",
        "Can the one-layer network approximate all three functions well? Why or why not?\n",
        "\n",
        "What advantages does the two-layer network have compared to a one-layer network?\n",
        "\n",
        "How can we determine the appropriate number of hidden neurons?\n",
        "When looking at the example plots in the OLAT, how many hidden neurons do we need in order to approximate the functions? Is there any difference between the three target functions?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk3ccyq7Smtx"
      },
      "source": [
        "Answer:\n",
        "\n",
        "The one-layer network (linear) cannot fit the nonlinear functions well.\n",
        "\n",
        "The one-layer network with activation can approximate smooth functions better but is still limited, only one change of direction can be approximated.\n",
        "\n",
        "The two-layer network can approximate complex functions, but the number of hidden neurons should be chosen carefully.\n",
        "\n",
        "The number of hidden neurons depends on the complexity of the function:\n",
        "\n",
        "$X_1$: 10\n",
        "\n",
        "$X_2$: 2\n",
        "\n",
        "$X_3$: More complex functions usually require more units: here we take 80.\n",
        "\n",
        "More neurons allow for better approximation but increase computation cost and risk of overfitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki1cAn6zSvj2"
      },
      "source": [
        "#### Task 1.3: Network Performance\n",
        "\n",
        "If the network struggles to approximate a function well, what are some possible reasons?\n",
        "\n",
        "How can we improve the network's performance?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNzD6ysHS2tH"
      },
      "source": [
        "Answer:\n",
        "\n",
        "Reasons:\n",
        "1. Too few hidden neurons (underfitting).\n",
        "2. Poor weight initialization.\n",
        "3. Inappropriate activation function.\n",
        "\n",
        "Solutions:\n",
        "1. Changing the number of hidden neurons.\n",
        "2. Adjusting the learning rate, e.g. use adaptive learning rate techniques\n",
        "3. Choosing a different activation function.\n",
        "4. Modifying the loss function.\n",
        "5. Adding output normalization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPUvWGhybtv9"
      },
      "source": [
        "### Derivatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9ibD4zrPOvE"
      },
      "source": [
        "#### Task 1.4: Activation Function\n",
        "\n",
        "Given the hyperbolic tangent ($\\tanh$) activation function as:\n",
        "\n",
        "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
        "\n",
        "Prove:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial x} \\tanh(x) = 1 - \\tanh^2(x)$$\n",
        "\n",
        "Hint: Apply the derivative rules as defined in the Lecture:\n",
        "* Quotient rule\n",
        "* Sum rule\n",
        "* Exponential rule\n",
        "\n",
        "Also, avoid factoring out parentheses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL5vqduaPZnf"
      },
      "source": [
        "Answer:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\frac{\\partial}{\\partial x} \\tanh(x) &= \\frac{\\partial  \\frac{e^x - e^{-x}}{e^x + e^{-x}}}{\\partial x} \\\\[6ex]\n",
        "\\text{(quotient rule)} &= \\frac{\\frac{\\partial(e^x - e^{-x})}{x} (e^x + e^{-x}) - \\frac{\\partial(e^x + e^{-x})}{x} (e^x - e^{-x})}{(e^x + e^{-x})^2} \\\\[6ex]\n",
        "\\text{(exponential rule)} &= \\frac{(e^x + e^{-x})(e^x + e^{-x}) - (e^x - e^{-x})(e^x - e^{-x})}{(e^x + e^{-x})^2}\\\\[6ex]\n",
        " &= \\frac{(e^x + e^{-x})(e^x + e^{-x})}{(e^x + e^{-x})^2} - \\frac{(e^x - e^{-x})(e^x - e^{-x})}{(e^x + e^{-x})^2} \\\\[6ex]\n",
        " &= \\frac{(e^x + e^{-x})^2}{(e^x + e^{-x})^2} - \\frac{(e^x - e^{-x})^2}{(e^x + e^{-x})^2} \\\\[6ex]\n",
        " &= 1 - \\left(\\frac{e^x - e^{-x}}{e^x + e^{-x}}\\right)^2 = 1 - \\tanh^2(x)\n",
        "\\end{aligned}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC041GO7PzIP"
      },
      "source": [
        "#### Task 1.5: Weight Decay\n",
        "\n",
        "Consider a loss function with L2 regularization (weight decay):\n",
        "$$\n",
        "L'(\\theta) = L(\\theta) + \\frac{\\lambda}{2} \\|\\theta\\|^2\n",
        "$$\n",
        "\n",
        "Compute its derivative with respect to $\\theta$: $$\\frac{\\partial}{\\partial \\theta} L'(\\theta)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcfZ82qZP7hJ"
      },
      "source": [
        "Answer:\n",
        "\n",
        "Taking the derivative,\n",
        "$\n",
        "\\frac{\\partial}{\\partial \\theta} L'(\\theta) = \\frac{\\partial}{\\partial \\theta} L(\\theta) + \\frac{\\lambda}{2} \\frac{\\partial}{\\partial \\theta} \\sum_i \\theta_i^2\n",
        "$\n",
        "\n",
        "Since $\\frac{\\partial}{\\partial \\theta} \\sum_i \\theta_i^2 = 2\\theta$, we get:\n",
        "$\n",
        "\\nabla_\\theta L' = \\nabla_\\theta L + \\lambda \\theta\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOiw6bNJQG5E"
      },
      "source": [
        "#### Task 1.6\n",
        "\n",
        "How large should an appropriate weight decay parameter $\\lambda$ as shown in Task 1.5 be? What would happen if $\\lambda$ is set too high or too low?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XKNMte5QZW7"
      },
      "source": [
        "Answer:\n",
        "\n",
        "Î» is usually set in a small range (e.g., $10^{-5}$ to $10^{-2}$).\n",
        "\n",
        "It should be selected to maintain the best trade-off between model complexity and generalization.\n",
        "\n",
        "If $\\lambda$ is too small, the model learns almost without regularization, meaning weights can grow large. This can lead to overfitting, where the model memorizes training data but performs poorly on unseen data.\n",
        "\n",
        "If $\\lambda$ is too large, the weight decay term dominates the optimization process, leading to:\n",
        "\n",
        "1. Excessive shrinkage of weights, causing the model to underfit the data.\n",
        "2. Loss of model capacity, where the neural network struggles to capture even simple relationships in the data.\n",
        "3. Degradation in performance, as the model's predictions become too simplistic (e.g., outputting the same value for all inputs).\n",
        "4. In extreme cases, setting $\\lambda$ too high can collapse all weights"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}